## Домашняя работа 1

[Ссылка](https://github.com/PoulYak/DeepSchoolMirea.git) на ноутбук, но кратко ответы распишу тут

### Домашнее задание 1
`neuron(neuron(1*x1+1*x2)-neuron(1*x1+1*x2-1))`

### Домашнее задание 2
Построенный график [здесь](https://drive.google.com/file/d/1pl2rMCkbdz6XhyEs5RvwxVWJ2G7E-L1N/view)

Полученные теоретические значения совпадают со значениями атрибутов

### Домашнее задание 3
**Полученные результаты**
- float32: GPU была почти полностью заполнена при размере $38*10^8$
- float64: GPU была почти полностью заполнена при размере $19*10^8$
- float16: GPU была почти полностью заполнена при размере $76*10^8$
- int32: GPU была почти полностью заполнена при размере $38*10^8$
- int64: GPU была почти полностью заполнена при размере $76*10^8$

Максимальный размер тензора для 64-битного типа данных в два раза меньше 32-битного и в 4 раза меньше 16-битного. И так же максимальный размер 16-битного тензора в 2 раза длинее 32-битного, что довольно логично. Различаются в $2^n$

### Домашнее задание 4

```
def allocate_empty_tensor(dim_size):
  a=torch.zeros(4096,dim_size,dtype=torch.float32,device='cuda')

for i in range(10):
    allocate_empty_tensor(10000000)
```

### Домашнее задание 5

Мной была задана переменная n, для простоты использованная в количестве входов и выходов в линейном слое. Подобрана n так, чтобы память была почти полностью заполнена, заполненность проверяем с помощью !nvidia-smi

Когда применили линейный слой к тензору, то код был прерван с ошибкой `CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling cublasCreate(handle)`